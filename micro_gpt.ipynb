{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "938bb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb5874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13b8d0fbb5d4d48888683c9ffdfe59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 93.6k/93.6k [00:00<00:00, 376kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef8f773b2834fb7a107856f986c3d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"zhyncs/sonnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46ba32",
   "metadata": {},
   "source": [
    "## Andrej uses a names dataset, but I will explore sonnet generation instead\n",
    "\n",
    "The task will be to generate (possibly Shakespearean-style) sonnets.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "There are a few approahces for tokenization. BPE is the real algorithm an LLm uses. For efficiency (and simplicity) in MicroGPT, I will use character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c603c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69799340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Love's fire heats water, water cools not love.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "247dd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = set()\n",
    "\n",
    "for row in train_ds:\n",
    "  chars_list = list(row['text'])\n",
    "  for c in chars_list:\n",
    "    unique_chars.add(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a911bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 59\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocab size:\", len(unique_chars))\n",
    "\n",
    "## Now convert this list of chars to a proper vocabulary\n",
    "## char_id -> char\n",
    "vocab = {index: char for index, char in enumerate(unique_chars)}\n",
    "\n",
    "## The tokenizer does the opposite: go from text to integers\n",
    "tokenizer = {char: index for index, char in enumerate(unique_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3c4534d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'X',\n",
       " 1: 'M',\n",
       " 2: 'B',\n",
       " 3: 'b',\n",
       " 4: 'a',\n",
       " 5: 'C',\n",
       " 6: 'F',\n",
       " 7: 'u',\n",
       " 8: 'T',\n",
       " 9: 'A',\n",
       " 10: '!',\n",
       " 11: 'r',\n",
       " 12: 'i',\n",
       " 13: ',',\n",
       " 14: 'E',\n",
       " 15: 'w',\n",
       " 16: 'z',\n",
       " 17: 'P',\n",
       " 18: 'm',\n",
       " 19: ':',\n",
       " 20: 's',\n",
       " 21: 'R',\n",
       " 22: 'N',\n",
       " 23: 'L',\n",
       " 24: 'G',\n",
       " 25: 'V',\n",
       " 26: 'K',\n",
       " 27: 'j',\n",
       " 28: 'h',\n",
       " 29: 'y',\n",
       " 30: ' ',\n",
       " 31: 'J',\n",
       " 32: 'q',\n",
       " 33: 'D',\n",
       " 34: ';',\n",
       " 35: 'n',\n",
       " 36: \"'\",\n",
       " 37: 'f',\n",
       " 38: '-',\n",
       " 39: 'O',\n",
       " 40: 'x',\n",
       " 41: 'o',\n",
       " 42: 'U',\n",
       " 43: 'p',\n",
       " 44: 't',\n",
       " 45: 'v',\n",
       " 46: 'I',\n",
       " 47: 'W',\n",
       " 48: 'Y',\n",
       " 49: 'l',\n",
       " 50: 'S',\n",
       " 51: 'g',\n",
       " 52: 'c',\n",
       " 53: 'd',\n",
       " 54: '.',\n",
       " 55: 'k',\n",
       " 56: 'e',\n",
       " 57: '?',\n",
       " 58: 'H'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9ea8f",
   "metadata": {},
   "source": [
    "## Build Autograd Now\n",
    "\n",
    "We want all basic operations in the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "25103905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  def __init__(self, val : float, name=\"\", op=\"\", children=[]):\n",
    "    self.val = val\n",
    "    self.name = name\n",
    "    self.children = children # should be ordered, not sets!\n",
    "    self.local_grads = [] # local gradient of current node wrt children\n",
    "    self.grad = 0 # total gradient (add to this)\n",
    "\n",
    "  def __repr__(self):\n",
    "    return {self.val}\n",
    "  \n",
    "  def __str__(self):\n",
    "    return (f\"{self.name}: {self.val}\")\n",
    "\n",
    "\n",
    "  # Overrie original add function for Value object\n",
    "  def __add__(self, other, op=\"+\", other_name=None):\n",
    "    if not isinstance(other, Value):\n",
    "      other = Value(other)\n",
    "    new_val = Value(self.val + other.val, children=set([self, other]))\n",
    "    new_val.local_grads = [1,1]\n",
    "    return new_val\n",
    "  \n",
    "  def __neg__(self):\n",
    "    new_val = Value(-self.val, children=[self])\n",
    "    new_val.local_grads = [-1]\n",
    "    return new_val\n",
    "  \n",
    "  def __sub__(self, other):\n",
    "    return self.__add__((-other), other_name= other.name)\n",
    "  \n",
    "  def __mul__(self, other, other_name=None):\n",
    "    if not isinstance(other, Value):\n",
    "      other = Value(other)\n",
    "    new_val = Value(self.val * other.val, children=set([self, other]))\n",
    "    new_val.local_grads = [other.val, self.val]\n",
    "    return new_val\n",
    "  \n",
    "  def __rmul__(self, other, other_name=None):\n",
    "    if not isinstance(other, Value):\n",
    "      other = Value(other)\n",
    "    new_val = Value(self.val * other.val, children=set([self, other]))\n",
    "    new_val.local_grads = [other.val, self.val]\n",
    "    return new_val\n",
    "  \n",
    "  def __pow__(self, exp : float, op=\"^\"):\n",
    "    new_val = Value(self.val ** exp, children=[self])\n",
    "    new_val.local_grads = [exp * self.val ** (exp - 1)]\n",
    "    return new_val\n",
    "  \n",
    "  def __truediv__(self, other, op=\"/\"):\n",
    "    if isinstance(other, (int, float)):\n",
    "      other = Value(float(other))\n",
    "    new_val = Value(self.val / other.val, children=[self, other])\n",
    "    new_val.local_grads = [1.0 / other.val, -self.val / (other.val * other.val)]\n",
    "    return new_val\n",
    "  \n",
    "  def exp(self):\n",
    "    new_val = Value(math.exp(self.val), children=[self])\n",
    "    new_val.local_grads = [math.exp(self.val)]\n",
    "    return new_val\n",
    "  \n",
    "  def log(self):\n",
    "    new_val = Value(math.log(self.val), children=[self])\n",
    "    new_val.local_grads = [1 / self.val]\n",
    "    return new_val\n",
    "  \n",
    "  def relu(self):\n",
    "    new_val = Value(max(0, self.val), children=[self])\n",
    "    new_val.local_grads = [float(self.val > 0)]\n",
    "    return new_val\n",
    "  \n",
    "  def backward(self):\n",
    "    # compute the gradient from the root\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    # first construct a topsort of the computational graph\n",
    "    def construct_toposort(cur):\n",
    "      if cur not in visited:\n",
    "        visited.add(cur)\n",
    "        for n in cur.children:\n",
    "          construct_toposort(n)\n",
    "        topo.append(cur)\n",
    "    \n",
    "    construct_toposort(self)\n",
    "    self.grad = 1\n",
    "    for v in reversed(topo):\n",
    "      for child, local_grads in zip(v.children, v.local_grads):\n",
    "        child.grad += local_grads * v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "641f6c65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__repr__ returned non-string (type set)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/cs224n/lib/python3.10/site-packages/IPython/core/formatters.py:770\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    763\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    764\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[1;32m    766\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[1;32m    767\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[1;32m    768\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[1;32m    769\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 770\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/anaconda3/envs/cs224n/lib/python3.10/site-packages/IPython/lib/pretty.py:419\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    409\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    410\u001b[0m                     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    411\u001b[0m                     \u001b[38;5;66;03m# check if cls defines __repr__\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(_safe_getattr(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    418\u001b[0m                 ):\n\u001b[0;32m--> 419\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cs224n/lib/python3.10/site-packages/IPython/lib/pretty.py:794\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "\u001b[0;31mTypeError\u001b[0m: __repr__ returned non-string (type set)"
     ]
    }
   ],
   "source": [
    "inf = Value(float('-inf'))\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3c04a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(5, \"a\")\n",
    "b = Value(3, \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "23afa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = c**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "0617d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "8876af5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4e1ae69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c8e4dac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "245581a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "54cb22a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4045663001.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[210], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    so grad(d) = 1\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = 5\n",
    "b = 3\n",
    "c = a + b # 8\n",
    "d = c^3\n",
    "\n",
    "so grad(d) = 1\n",
    "grad (c) = 3 c^2\n",
    "grad(b) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f53c0e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "2.0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(3.0)\n",
    "c = a * b       # c = 6.0\n",
    "L = c + a       # L = 8.0\n",
    "L.backward()\n",
    "print(a.grad)   # 4.0 (dL/da = b + 1 = 3 + 1, via both paths)\n",
    "print(b.grad)   # 2.0 (dL/db = a = 2)\n",
    "print(c.grad)   \n",
    "print(L.grad)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58c223",
   "metadata": {},
   "source": [
    "## Now that AutoGrad is working, can actually define our model!\n",
    "\n",
    "This won't be too big of a model (just overfit to sonnets), so will have:\n",
    "- embedding dimension: 16\n",
    "- attention heads: 4\n",
    "- layers: 1\n",
    "- block size: 16 # max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "40a5b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 16\n",
    "attn_heads = 4\n",
    "layers = 1\n",
    "block_size = 16\n",
    "hidden_dim = int(embed_dim / attn_heads) # dimension of each head\n",
    "# define some matrix\n",
    "vocab_size = len(vocab)\n",
    "def create_matrix(in_dim, out_dim, std=0.08):\n",
    "  # creates a matrix of size in_dim x out_dim\n",
    "  return [[Value(random.gauss(0, std)) for _ in range(out_dim)] for _ in range(in_dim)]\n",
    "\n",
    "state_dict = {}\n",
    "for i in range(layers):\n",
    "  # Projects embedding -> Query vectors\n",
    "  state_dict[f'layer_{i}_attn_wq'] = create_matrix(embed_dim, embed_dim)\n",
    "  # Projects embedding -> Key vectors\n",
    "  state_dict[f'layer_{i}_attn_wk'] = create_matrix(embed_dim, embed_dim)\n",
    "  # Projects embedding -> Value vectors\n",
    "  state_dict[f'layer_{i}_attn_wv'] = create_matrix(embed_dim, embed_dim)\n",
    "  # Final projection after concatenating all heads.\n",
    "  state_dict[f'layer_{i}_attn_wo'] = create_matrix(embed_dim, embed_dim)\n",
    "  # Standard MLP: Linear(embed_dim -> 4*embed_dim); GELU; Linear(4*embed_dim -> embed_dim)\n",
    "  state_dict[f'layer_{i}_fc1'] = create_matrix(embed_dim, 4 * embed_dim)\n",
    "  state_dict[f'layer_{i}_fc2'] = create_matrix(4 * embed_dim, embed_dim)\n",
    "\n",
    "# Used to map final hidden state -> logits over vocabulary:\n",
    "state_dict[f'output_head'] = create_matrix(vocab_size, embed_dim)\n",
    "# This is positional embedding\n",
    "state_dict[f'pos_embedding'] = create_matrix(block_size, embed_dim)\n",
    "# this is token embedding\n",
    "state_dict[f'token_embedding'] = create_matrix(vocab_size, embed_dim)\n",
    "\n",
    "params = [p for mat in state_dict.values() for row in mat for p in row]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2693efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA:\n",
    "  def __init__(self, wq, wk, wv, wo, config):\n",
    "    self.wq = wq\n",
    "    self.wk = wk\n",
    "    self.wv = wv\n",
    "    self.wo = wo\n",
    "    self.config = config\n",
    "  \n",
    "  def forward(self, input, attn_mask):\n",
    "    attn_weights = (self.wq @ self.wk.T) / math.sqrt(self.config.hidden_dim)\n",
    "\n",
    "    \n",
    "\n",
    "    masked_attn_weights = attn_weights +  attn_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1ea26282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(W, x):\n",
    "  return [sum((w_i * x_i for w_i, x_i in zip(w_row, x)), Value(0.0)) for w_row in W]\n",
    "\n",
    "def softmax(input):\n",
    "  denom = sum((Value.exp(x) for x in input), Value(0.0))\n",
    "  logits = [x.exp() / denom for x in input]\n",
    "  return logits\n",
    "\n",
    "def rmsnorm(x):\n",
    "  ms = sum((xi * xi for xi in x), Value(0.0)) / Value(float(len(x)))\n",
    "  scale = (ms + 1e-5) ** -0.5\n",
    "  return [xi * scale for xi in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "8e474d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = [[Value(float('-inf')) for _ in range(hidden_dim)] for _ in range(hidden_dim)]\n",
    "for i in range(hidden_dim):\n",
    "  for j in range(i):\n",
    "    attn_mask[i][j] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "802e57c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(dim, i):\n",
    "  output = [Value(0) for _ in range(dim)]\n",
    "  output[i] = 1\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a517dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(token_id, pos_id, keys, values): # add attention mask?\n",
    "  # one_hot_token_id = one_hot(vocab_size, token_id)\n",
    "  # token_emb = linear(state_dict[f'token_embedding'], one_hot_token_id)\n",
    "\n",
    "  # one_hot_pos_id = one_hot(block_size, pos_id)\n",
    "  # pos_emb = linear(state_dict[f'pos_embedding'], one_hot_pos_id)\n",
    "\n",
    "  token_emb = state_dict['token_embedding'][token_id]\n",
    "  pos_emb = state_dict['pos_embedding'][pos_id]\n",
    "  embedding = token_emb + pos_emb\n",
    "  embedding = rmsnorm(embedding)\n",
    "\n",
    "  input = embedding\n",
    "  output = None\n",
    "  for i in range(layers):\n",
    "    residual = input\n",
    "    # MHA\n",
    "    Q = linear(state_dict[f'layer_{i}_attn_wq'], input)\n",
    "    K = linear(state_dict[f'layer_{i}_attn_wk'], input)\n",
    "    V = linear(state_dict[f'layer_{i}_attn_wv'], input)\n",
    "\n",
    "    keys.append(K)\n",
    "    values.append(V)\n",
    "\n",
    "    x_attn = []\n",
    "    for h in range(attn_heads):\n",
    "      hs = h * hidden_dim\n",
    "      q_h = Q[hs: hs + hidden_dim]\n",
    "      k_h = [ki[hs: hs + hidden_dim] for ki in keys[i]]\n",
    "      v_h = [vi[hs: hs + hidden_dim] for vi in values[i]]\n",
    "      attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(hidden_dim)) / math.sqrt(hidden_dim) for t in range(len(k_h))]\n",
    "      attn_weights = softmax(attn_logits)\n",
    "      head_out = [sum(attn_weights[i] * v_h[t][j] for t in range(len(v_h))) for j in range(hidden_dim)]\n",
    "\n",
    "      x_attn.extend(head_out)\n",
    "\n",
    "    \n",
    "    x_proj = linear(state_dict[f'layer_{i}_attn_wo'], x_attn)\n",
    "    x = [x_i + r for x_i, r in zip(x_proj, residual)]\n",
    "    x_residual = x\n",
    "    x = rmsnorm(x)\n",
    "\n",
    "    ## Feed Forward\n",
    "\n",
    "    l1 =  linear(state_dict[f'layer_{i}_fc1'], x)\n",
    "    l1_relu = [x.relu() for x in l1]\n",
    "    l2 = linear(state_dict[f'layer_{i}_fc1'], l1_relu)\n",
    "    x = l2 + x_residual\n",
    "\n",
    "    input = x\n",
    "    output = x\n",
    "  logits = linear(state_dict['output_head'], output)\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "c969de5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    1 / 1000 | loss 4.0827\n",
      "step    2 / 1000 | loss 4.0744\n",
      "step    3 / 1000 | loss 4.0704\n",
      "step    4 / 1000 | loss 4.0728\n",
      "step    5 / 1000 | loss 4.0646\n",
      "step    6 / 1000 | loss 4.0649\n",
      "step    7 / 1000 | loss 4.0658\n",
      "step    8 / 1000 | loss 4.0590\n",
      "step    9 / 1000 | loss 4.0505\n",
      "step   10 / 1000 | loss 4.0457\n",
      "step   11 / 1000 | loss 4.0456\n",
      "step   12 / 1000 | loss 4.0434\n",
      "step   13 / 1000 | loss 4.0320\n",
      "step   14 / 1000 | loss 4.0300\n",
      "step   15 / 1000 | loss 4.0291\n",
      "step   16 / 1000 | loss 4.0213\n",
      "step   17 / 1000 | loss 4.0320\n",
      "step   18 / 1000 | loss 4.0170\n",
      "step   19 / 1000 | loss 3.9679\n",
      "step   20 / 1000 | loss 3.9652\n",
      "step   21 / 1000 | loss 3.9196\n",
      "step   22 / 1000 | loss 3.9537\n",
      "step   23 / 1000 | loss 3.9232\n",
      "step   24 / 1000 | loss 3.8676\n",
      "step   25 / 1000 | loss 3.9383\n",
      "step   26 / 1000 | loss 3.8293\n",
      "step   27 / 1000 | loss 3.7553\n",
      "step   28 / 1000 | loss 3.7911\n",
      "step   29 / 1000 | loss 3.8245\n",
      "step   30 / 1000 | loss 3.5528\n",
      "step   31 / 1000 | loss 3.6966\n",
      "step   32 / 1000 | loss 3.6148\n",
      "step   33 / 1000 | loss 3.4747\n",
      "step   34 / 1000 | loss 3.4378\n",
      "step   35 / 1000 | loss 3.2619\n",
      "step   36 / 1000 | loss 3.6403\n",
      "step   37 / 1000 | loss 3.2966\n",
      "step   38 / 1000 | loss 3.4547\n",
      "step   39 / 1000 | loss 3.1724\n",
      "step   40 / 1000 | loss 3.4492\n",
      "step   41 / 1000 | loss 3.3118\n",
      "step   42 / 1000 | loss 3.1178\n",
      "step   43 / 1000 | loss 3.4770\n",
      "step   44 / 1000 | loss 2.9801\n",
      "step   45 / 1000 | loss 3.4144\n",
      "step   46 / 1000 | loss 3.5866\n",
      "step   47 / 1000 | loss 3.2309\n",
      "step   48 / 1000 | loss 3.2183\n",
      "step   49 / 1000 | loss 3.3243\n",
      "step   50 / 1000 | loss 3.4756\n",
      "step   51 / 1000 | loss 3.0835\n",
      "step   52 / 1000 | loss 2.7569\n",
      "step   53 / 1000 | loss 3.2207\n",
      "step   54 / 1000 | loss 3.1270\n",
      "step   55 / 1000 | loss 3.1324\n",
      "step   56 / 1000 | loss 3.4032\n",
      "step   57 / 1000 | loss 2.8568\n",
      "step   58 / 1000 | loss 3.5931\n",
      "step   59 / 1000 | loss 3.2591\n",
      "step   60 / 1000 | loss 3.1535\n",
      "step   61 / 1000 | loss 3.4041\n",
      "step   62 / 1000 | loss 3.0809\n",
      "step   63 / 1000 | loss 3.5849\n",
      "step   64 / 1000 | loss 3.3781\n",
      "step   65 / 1000 | loss 3.0276\n",
      "step   66 / 1000 | loss 3.4101\n",
      "step   67 / 1000 | loss 3.2347\n",
      "step   68 / 1000 | loss 2.9768\n",
      "step   69 / 1000 | loss 3.2409\n",
      "step   70 / 1000 | loss 2.7883\n",
      "step   71 / 1000 | loss 2.8511\n",
      "step   72 / 1000 | loss 2.9682\n",
      "step   73 / 1000 | loss 3.4619\n",
      "step   74 / 1000 | loss 2.9346\n",
      "step   75 / 1000 | loss 2.7253\n",
      "step   76 / 1000 | loss 3.0448\n",
      "step   77 / 1000 | loss 3.1231\n",
      "step   78 / 1000 | loss 2.9387\n",
      "step   79 / 1000 | loss 2.7532\n",
      "step   80 / 1000 | loss 2.7080\n",
      "step   81 / 1000 | loss 3.2527\n",
      "step   82 / 1000 | loss 2.9259\n",
      "step   83 / 1000 | loss 3.0723\n",
      "step   84 / 1000 | loss 3.1372\n",
      "step   85 / 1000 | loss 2.9734\n",
      "step   86 / 1000 | loss 3.1984\n",
      "step   87 / 1000 | loss 2.8922\n",
      "step   88 / 1000 | loss 3.2911\n",
      "step   89 / 1000 | loss 3.7145\n",
      "step   90 / 1000 | loss 3.3182\n",
      "step   91 / 1000 | loss 3.1667\n",
      "step   92 / 1000 | loss 2.8438\n",
      "step   93 / 1000 | loss 2.9700\n",
      "step   94 / 1000 | loss 3.0303\n",
      "step   95 / 1000 | loss 3.1257\n",
      "step   96 / 1000 | loss 2.9448\n",
      "step   97 / 1000 | loss 2.9408\n",
      "step   98 / 1000 | loss 3.0874\n",
      "step   99 / 1000 | loss 3.0667\n",
      "step  100 / 1000 | loss 2.8588\n",
      "step  101 / 1000 | loss 2.5860\n",
      "step  102 / 1000 | loss 3.0715\n",
      "step  103 / 1000 | loss 2.9330\n",
      "step  104 / 1000 | loss 3.4269\n",
      "step  105 / 1000 | loss 2.9973\n",
      "step  106 / 1000 | loss 2.8546\n",
      "step  107 / 1000 | loss 3.0379\n",
      "step  108 / 1000 | loss 3.0282\n",
      "step  109 / 1000 | loss 3.0754\n",
      "step  110 / 1000 | loss 2.9493\n",
      "step  111 / 1000 | loss 2.6715\n",
      "step  112 / 1000 | loss 2.5267\n",
      "step  113 / 1000 | loss 2.9078\n",
      "step  114 / 1000 | loss 2.9623\n",
      "step  115 / 1000 | loss 2.9993\n",
      "step  116 / 1000 | loss 3.0653\n",
      "step  117 / 1000 | loss 2.9888\n",
      "step  118 / 1000 | loss 2.8689\n",
      "step  119 / 1000 | loss 3.7676\n",
      "step  120 / 1000 | loss 3.1860\n",
      "step  121 / 1000 | loss 3.1918\n",
      "step  122 / 1000 | loss 2.7570\n",
      "step  123 / 1000 | loss 3.2734\n",
      "step  124 / 1000 | loss 3.4806\n",
      "step  125 / 1000 | loss 2.9508\n",
      "step  126 / 1000 | loss 3.0289\n",
      "step  127 / 1000 | loss 3.3029\n",
      "step  128 / 1000 | loss 3.1411\n",
      "step  129 / 1000 | loss 3.0246\n",
      "step  130 / 1000 | loss 2.4924\n",
      "step  131 / 1000 | loss 2.8043\n",
      "step  132 / 1000 | loss 3.2154\n",
      "step  133 / 1000 | loss 3.1500\n",
      "step  134 / 1000 | loss 3.0698\n",
      "step  135 / 1000 | loss 2.6934\n",
      "step  136 / 1000 | loss 2.7021\n",
      "step  137 / 1000 | loss 3.0425\n",
      "step  138 / 1000 | loss 2.8053\n",
      "step  139 / 1000 | loss 2.6402\n",
      "step  140 / 1000 | loss 3.1004\n",
      "step  141 / 1000 | loss 2.4344\n",
      "step  142 / 1000 | loss 2.8309\n",
      "step  143 / 1000 | loss 2.9243\n",
      "step  144 / 1000 | loss 2.8621\n",
      "step  145 / 1000 | loss 3.4151\n",
      "step  146 / 1000 | loss 2.6282\n",
      "step  147 / 1000 | loss 3.0896\n",
      "step  148 / 1000 | loss 3.0008\n",
      "step  149 / 1000 | loss 3.3436\n",
      "step  150 / 1000 | loss 3.1807\n",
      "step  151 / 1000 | loss 3.0052\n",
      "step  152 / 1000 | loss 3.3387\n",
      "step  153 / 1000 | loss 2.7671\n",
      "step  154 / 1000 | loss 2.9811\n",
      "step  155 / 1000 | loss 3.9045\n",
      "step  156 / 1000 | loss 2.5364\n",
      "step  157 / 1000 | loss 3.6656\n",
      "step  158 / 1000 | loss 3.0097\n",
      "step  159 / 1000 | loss 2.9770\n",
      "step  160 / 1000 | loss 2.9094\n",
      "step  161 / 1000 | loss 3.6056\n",
      "step  162 / 1000 | loss 2.7244\n",
      "step  163 / 1000 | loss 3.0707\n",
      "step  164 / 1000 | loss 2.7017\n",
      "step  165 / 1000 | loss 2.7971\n",
      "step  166 / 1000 | loss 2.9034\n",
      "step  167 / 1000 | loss 3.4366\n",
      "step  168 / 1000 | loss 2.8506\n",
      "step  169 / 1000 | loss 2.9102\n",
      "step  170 / 1000 | loss 3.2367\n",
      "step  171 / 1000 | loss 3.2453\n",
      "step  172 / 1000 | loss 3.0119\n",
      "step  173 / 1000 | loss 2.7917\n",
      "step  174 / 1000 | loss 2.9524\n",
      "step  175 / 1000 | loss 3.2766\n",
      "step  176 / 1000 | loss 3.1355\n",
      "step  177 / 1000 | loss 2.7238\n",
      "step  178 / 1000 | loss 3.2365\n",
      "step  179 / 1000 | loss 3.0447\n",
      "step  180 / 1000 | loss 2.9219\n",
      "step  181 / 1000 | loss 3.0453\n",
      "step  182 / 1000 | loss 3.0205\n",
      "step  183 / 1000 | loss 3.0240\n",
      "step  184 / 1000 | loss 3.1949\n",
      "step  185 / 1000 | loss 2.4227\n",
      "step  186 / 1000 | loss 3.3498\n",
      "step  187 / 1000 | loss 4.0343\n",
      "step  188 / 1000 | loss 2.7956\n",
      "step  189 / 1000 | loss 3.1159\n",
      "step  190 / 1000 | loss 2.9971\n",
      "step  191 / 1000 | loss 2.9096\n",
      "step  192 / 1000 | loss 3.0054\n",
      "step  193 / 1000 | loss 2.9393\n",
      "step  194 / 1000 | loss 3.0656\n",
      "step  195 / 1000 | loss 2.5863\n",
      "step  196 / 1000 | loss 3.0365\n",
      "step  197 / 1000 | loss 3.7876\n",
      "step  198 / 1000 | loss 3.2064\n",
      "step  199 / 1000 | loss 2.7175\n",
      "step  200 / 1000 | loss 2.7798\n",
      "step  201 / 1000 | loss 4.0892\n",
      "step  202 / 1000 | loss 3.1592\n",
      "step  203 / 1000 | loss 2.9656\n",
      "step  204 / 1000 | loss 3.1613\n",
      "step  205 / 1000 | loss 2.7981\n",
      "step  206 / 1000 | loss 3.0111\n",
      "step  207 / 1000 | loss 3.6248\n",
      "step  208 / 1000 | loss 3.0447\n",
      "step  209 / 1000 | loss 3.2264\n",
      "step  210 / 1000 | loss 3.2464\n",
      "step  211 / 1000 | loss 2.9137\n",
      "step  212 / 1000 | loss 3.3520\n",
      "step  213 / 1000 | loss 3.4031\n",
      "step  214 / 1000 | loss 3.0714\n",
      "step  215 / 1000 | loss 3.1802\n",
      "step  216 / 1000 | loss 3.3111\n",
      "step  217 / 1000 | loss 3.2395\n",
      "step  218 / 1000 | loss 3.3844\n",
      "step  219 / 1000 | loss 3.0486\n",
      "step  220 / 1000 | loss 3.6390\n",
      "step  221 / 1000 | loss 2.8125\n",
      "step  222 / 1000 | loss 3.4649\n",
      "step  223 / 1000 | loss 3.6736\n",
      "step  224 / 1000 | loss 3.3962\n",
      "step  225 / 1000 | loss 3.1444\n",
      "step  226 / 1000 | loss 3.1715\n",
      "step  227 / 1000 | loss 3.2854\n",
      "step  228 / 1000 | loss 3.0878\n",
      "step  229 / 1000 | loss 3.6657\n",
      "step  230 / 1000 | loss 3.1121\n",
      "step  231 / 1000 | loss 2.9816\n",
      "step  232 / 1000 | loss 3.4193\n",
      "step  233 / 1000 | loss 2.9596\n",
      "step  234 / 1000 | loss 3.6344\n",
      "step  235 / 1000 | loss 3.3148\n",
      "step  236 / 1000 | loss 2.8851\n",
      "step  237 / 1000 | loss 2.7842\n",
      "step  238 / 1000 | loss 2.9738\n",
      "step  239 / 1000 | loss -0.0000\n",
      "step  240 / 1000 | loss 12.6351\n",
      "step  241 / 1000 | loss -0.0000\n",
      "step  242 / 1000 | loss 3.7834\n",
      "step  243 / 1000 | loss 2.8945\n",
      "step  244 / 1000 | loss 3.0905\n",
      "step  245 / 1000 | loss 3.3540\n",
      "step  246 / 1000 | loss 3.1786\n",
      "step  247 / 1000 | loss 3.2158\n",
      "step  248 / 1000 | loss 3.2008\n",
      "step  249 / 1000 | loss 3.1191\n",
      "step  250 / 1000 | loss 2.8150\n",
      "step  251 / 1000 | loss 3.1128\n",
      "step  252 / 1000 | loss 4.7589\n",
      "step  253 / 1000 | loss 2.9225\n",
      "step  254 / 1000 | loss 3.3026\n",
      "step  255 / 1000 | loss 2.9639\n",
      "step  256 / 1000 | loss 4.3974\n",
      "step  257 / 1000 | loss 3.3290\n",
      "step  258 / 1000 | loss 3.6113\n",
      "step  259 / 1000 | loss 3.4582\n",
      "step  260 / 1000 | loss 3.4447\n",
      "step  261 / 1000 | loss 3.3612\n",
      "step  262 / 1000 | loss 3.1140\n",
      "step  263 / 1000 | loss 4.3189\n",
      "step  264 / 1000 | loss 3.3211\n",
      "step  265 / 1000 | loss 3.3897\n",
      "step  266 / 1000 | loss 3.6554\n",
      "step  267 / 1000 | loss 3.3816\n",
      "step  268 / 1000 | loss 2.9736\n",
      "step  269 / 1000 | loss 3.1672\n",
      "step  270 / 1000 | loss 3.4089\n",
      "step  271 / 1000 | loss 3.2964\n",
      "step  272 / 1000 | loss 3.2081\n",
      "step  273 / 1000 | loss 3.0820\n",
      "step  274 / 1000 | loss 2.9298\n",
      "step  275 / 1000 | loss 3.4076\n",
      "step  276 / 1000 | loss 3.1729\n",
      "step  277 / 1000 | loss 3.1477\n",
      "step  278 / 1000 | loss 2.7640\n",
      "step  279 / 1000 | loss 4.5959\n",
      "step  280 / 1000 | loss 3.0145\n",
      "step  281 / 1000 | loss 2.8037\n",
      "step  282 / 1000 | loss 3.1573\n",
      "step  283 / 1000 | loss 2.8723\n",
      "step  284 / 1000 | loss 2.9050\n",
      "step  285 / 1000 | loss 3.2425\n",
      "step  286 / 1000 | loss 3.4893\n",
      "step  287 / 1000 | loss 3.1579\n",
      "step  288 / 1000 | loss 3.4381\n",
      "step  289 / 1000 | loss 2.7314\n",
      "step  290 / 1000 | loss 5.0012\n",
      "step  291 / 1000 | loss 3.3108\n",
      "step  292 / 1000 | loss 3.5276\n",
      "step  293 / 1000 | loss 3.2228\n",
      "step  294 / 1000 | loss 3.0047\n",
      "step  295 / 1000 | loss 3.4643\n",
      "step  296 / 1000 | loss 3.1323\n",
      "step  297 / 1000 | loss 3.5106\n",
      "step  298 / 1000 | loss 3.3790\n",
      "step  299 / 1000 | loss 3.6646\n",
      "step  300 / 1000 | loss 3.2513\n",
      "step  301 / 1000 | loss 4.2057\n",
      "step  302 / 1000 | loss 3.0035\n",
      "step  303 / 1000 | loss 3.1677\n",
      "step  304 / 1000 | loss 2.9997\n",
      "step  305 / 1000 | loss 3.7394\n",
      "step  306 / 1000 | loss 3.0672\n",
      "step  307 / 1000 | loss 3.4180\n",
      "step  308 / 1000 | loss 3.2437\n",
      "step  309 / 1000 | loss 3.2427\n",
      "step  310 / 1000 | loss 3.0925\n",
      "step  311 / 1000 | loss 3.2282\n",
      "step  312 / 1000 | loss 3.3626\n",
      "step  313 / 1000 | loss 3.3187\n",
      "step  314 / 1000 | loss 3.3454\n",
      "step  315 / 1000 | loss 3.2660\n",
      "step  316 / 1000 | loss 3.5828\n",
      "step  317 / 1000 | loss 3.0620\n",
      "step  318 / 1000 | loss 2.6158\n",
      "step  319 / 1000 | loss 3.8274\n",
      "step  320 / 1000 | loss 3.4051\n",
      "step  321 / 1000 | loss 3.1289\n",
      "step  322 / 1000 | loss 3.1740\n",
      "step  323 / 1000 | loss 2.4858\n",
      "step  324 / 1000 | loss 3.0147\n",
      "step  325 / 1000 | loss 3.3178\n",
      "step  326 / 1000 | loss 2.9281\n",
      "step  327 / 1000 | loss 3.0225\n",
      "step  328 / 1000 | loss 3.3964\n",
      "step  329 / 1000 | loss 3.3175\n",
      "step  330 / 1000 | loss 3.6498\n",
      "step  331 / 1000 | loss 3.1441\n",
      "step  332 / 1000 | loss 3.5198\n",
      "step  333 / 1000 | loss 2.7809\n",
      "step  334 / 1000 | loss 3.7547\n",
      "step  335 / 1000 | loss 2.9107\n",
      "step  336 / 1000 | loss 3.4280\n",
      "step  337 / 1000 | loss 3.5786\n",
      "step  338 / 1000 | loss 2.9627\n",
      "step  339 / 1000 | loss 3.3207\n",
      "step  340 / 1000 | loss 2.9685\n",
      "step  341 / 1000 | loss 3.7961\n",
      "step  342 / 1000 | loss 4.0496\n",
      "step  343 / 1000 | loss 3.7248\n",
      "step  344 / 1000 | loss 3.2113\n",
      "step  345 / 1000 | loss 3.5761\n",
      "step  346 / 1000 | loss 3.2896\n",
      "step  347 / 1000 | loss 3.0819\n",
      "step  348 / 1000 | loss 3.4247\n",
      "step  349 / 1000 | loss 3.1072\n",
      "step  350 / 1000 | loss 3.8935\n",
      "step  351 / 1000 | loss 3.1520\n",
      "step  352 / 1000 | loss 3.3309\n",
      "step  353 / 1000 | loss 3.8081\n",
      "step  354 / 1000 | loss 3.0194\n",
      "step  355 / 1000 | loss 3.0630\n",
      "step  356 / 1000 | loss 3.2585\n",
      "step  357 / 1000 | loss 2.7948\n",
      "step  358 / 1000 | loss 2.9399\n",
      "step  359 / 1000 | loss 3.7442\n",
      "step  360 / 1000 | loss 3.1644\n",
      "step  361 / 1000 | loss 2.9212\n",
      "step  362 / 1000 | loss 3.1464\n",
      "step  363 / 1000 | loss 3.5977\n",
      "step  364 / 1000 | loss 3.2441\n",
      "step  365 / 1000 | loss 3.0943\n",
      "step  366 / 1000 | loss 3.2182\n",
      "step  367 / 1000 | loss 3.3816\n",
      "step  368 / 1000 | loss 2.9251\n",
      "step  369 / 1000 | loss 2.8342\n",
      "step  370 / 1000 | loss 3.1209\n",
      "step  371 / 1000 | loss 2.9120\n",
      "step  372 / 1000 | loss 3.0429\n",
      "step  373 / 1000 | loss 3.3296\n",
      "step  374 / 1000 | loss 3.5604\n",
      "step  375 / 1000 | loss 3.4626\n",
      "step  376 / 1000 | loss 2.8978\n",
      "step  377 / 1000 | loss 3.1117\n",
      "step  378 / 1000 | loss 4.2223\n",
      "step  379 / 1000 | loss 3.8829\n",
      "step  380 / 1000 | loss 4.1000\n",
      "step  381 / 1000 | loss 2.8350\n",
      "step  382 / 1000 | loss 3.5892\n",
      "step  383 / 1000 | loss 3.3070\n",
      "step  384 / 1000 | loss 3.6590\n",
      "step  385 / 1000 | loss 3.1220\n",
      "step  386 / 1000 | loss 3.0701\n",
      "step  387 / 1000 | loss 3.1547\n",
      "step  388 / 1000 | loss 2.7502\n",
      "step  389 / 1000 | loss 3.3184\n",
      "step  390 / 1000 | loss 3.3079\n",
      "step  391 / 1000 | loss 3.7365\n",
      "step  392 / 1000 | loss 3.1792\n",
      "step  393 / 1000 | loss 3.6258\n",
      "step  394 / 1000 | loss 3.6864\n",
      "step  395 / 1000 | loss 3.3011\n",
      "step  396 / 1000 | loss 3.3801\n",
      "step  397 / 1000 | loss 3.2440\n",
      "step  398 / 1000 | loss 3.2009\n",
      "step  399 / 1000 | loss 3.2725\n",
      "step  400 / 1000 | loss 3.4664\n",
      "step  401 / 1000 | loss 3.7264\n",
      "step  402 / 1000 | loss 2.8628\n",
      "step  403 / 1000 | loss 3.2349\n",
      "step  404 / 1000 | loss 2.9323\n",
      "step  405 / 1000 | loss 3.0028\n",
      "step  406 / 1000 | loss 3.2958\n",
      "step  407 / 1000 | loss 3.7094\n",
      "step  408 / 1000 | loss 3.3646\n",
      "step  409 / 1000 | loss 3.1691\n",
      "step  410 / 1000 | loss 3.1662\n",
      "step  411 / 1000 | loss 3.5072\n",
      "step  412 / 1000 | loss 3.5658\n",
      "step  413 / 1000 | loss 3.0039\n",
      "step  414 / 1000 | loss 3.1310\n",
      "step  415 / 1000 | loss 3.5963\n",
      "step  416 / 1000 | loss 3.4449\n",
      "step  417 / 1000 | loss 3.8165\n",
      "step  418 / 1000 | loss 3.2556\n",
      "step  419 / 1000 | loss 3.1912\n",
      "step  420 / 1000 | loss 3.2249\n",
      "step  421 / 1000 | loss 3.1935\n",
      "step  422 / 1000 | loss 3.3574\n",
      "step  423 / 1000 | loss 3.0397\n",
      "step  424 / 1000 | loss 2.7932\n",
      "step  425 / 1000 | loss 3.3071\n",
      "step  426 / 1000 | loss 3.0170\n",
      "step  427 / 1000 | loss 2.7697\n",
      "step  428 / 1000 | loss 2.9696\n",
      "step  429 / 1000 | loss 3.2998\n",
      "step  430 / 1000 | loss 3.4361\n",
      "step  431 / 1000 | loss 3.1342\n",
      "step  432 / 1000 | loss 3.0322\n",
      "step  433 / 1000 | loss 3.2030\n",
      "step  434 / 1000 | loss 2.8824\n",
      "step  435 / 1000 | loss 3.1303\n",
      "step  436 / 1000 | loss 3.0632\n",
      "step  437 / 1000 | loss 3.0530\n",
      "step  438 / 1000 | loss 3.2906\n",
      "step  439 / 1000 | loss 3.5829\n",
      "step  440 / 1000 | loss 2.8744\n",
      "step  441 / 1000 | loss 3.0770\n",
      "step  442 / 1000 | loss 3.3588\n",
      "step  443 / 1000 | loss 3.2539\n",
      "step  444 / 1000 | loss 2.9195\n",
      "step  445 / 1000 | loss 3.1416\n",
      "step  446 / 1000 | loss 3.7380\n",
      "step  447 / 1000 | loss 3.8982\n",
      "step  448 / 1000 | loss 2.7459\n",
      "step  449 / 1000 | loss 3.1865\n",
      "step  450 / 1000 | loss 3.0852\n",
      "step  451 / 1000 | loss 2.5515\n",
      "step  452 / 1000 | loss 2.9237\n",
      "step  453 / 1000 | loss 3.0227\n",
      "step  454 / 1000 | loss 3.4284\n",
      "step  455 / 1000 | loss 3.2410\n",
      "step  456 / 1000 | loss 3.0595\n",
      "step  457 / 1000 | loss 3.6694\n",
      "step  458 / 1000 | loss 2.9590\n",
      "step  459 / 1000 | loss 3.0438\n",
      "step  460 / 1000 | loss 2.8661\n",
      "step  461 / 1000 | loss 3.4292\n",
      "step  462 / 1000 | loss 3.2222\n",
      "step  463 / 1000 | loss 3.3937\n",
      "step  464 / 1000 | loss 2.4745\n",
      "step  465 / 1000 | loss 3.0311\n",
      "step  466 / 1000 | loss 2.9445\n",
      "step  467 / 1000 | loss 2.8111\n",
      "step  468 / 1000 | loss 3.0517\n",
      "step  469 / 1000 | loss 2.9467\n",
      "step  470 / 1000 | loss 4.0724\n",
      "step  471 / 1000 | loss 2.9334\n",
      "step  472 / 1000 | loss 2.9143\n",
      "step  473 / 1000 | loss 2.7525\n",
      "step  474 / 1000 | loss 3.0262\n",
      "step  475 / 1000 | loss 3.2377\n",
      "step  476 / 1000 | loss 3.5585\n",
      "step  477 / 1000 | loss 2.6588\n",
      "step  478 / 1000 | loss 2.9887\n",
      "step  479 / 1000 | loss 2.9793\n",
      "step  480 / 1000 | loss 2.8813\n",
      "step  481 / 1000 | loss 2.6323\n",
      "step  482 / 1000 | loss 3.2003\n",
      "step  483 / 1000 | loss 3.1039\n",
      "step  484 / 1000 | loss 3.0345\n",
      "step  485 / 1000 | loss 4.4117\n",
      "step  486 / 1000 | loss 3.4532\n",
      "step  487 / 1000 | loss 3.5182\n",
      "step  488 / 1000 | loss 2.7499\n",
      "step  489 / 1000 | loss 2.8409\n",
      "step  490 / 1000 | loss 3.1824\n",
      "step  491 / 1000 | loss 3.3259\n",
      "step  492 / 1000 | loss 3.0748\n",
      "step  493 / 1000 | loss 2.9452\n",
      "step  494 / 1000 | loss 3.0712\n",
      "step  495 / 1000 | loss 3.0951\n",
      "step  496 / 1000 | loss 2.8178\n",
      "step  497 / 1000 | loss 2.6837\n",
      "step  498 / 1000 | loss 2.8308\n",
      "step  499 / 1000 | loss 3.0618\n",
      "step  500 / 1000 | loss 3.1601\n",
      "step  501 / 1000 | loss 3.3283\n",
      "step  502 / 1000 | loss 2.7833\n",
      "step  503 / 1000 | loss 3.3981\n",
      "step  504 / 1000 | loss 2.9523\n",
      "step  505 / 1000 | loss 2.7292\n",
      "step  506 / 1000 | loss 5.1741\n",
      "step  507 / 1000 | loss 3.0136\n",
      "step  508 / 1000 | loss 3.0229\n",
      "step  509 / 1000 | loss 2.5282\n",
      "step  510 / 1000 | loss 2.8267\n",
      "step  511 / 1000 | loss 3.1589\n",
      "step  512 / 1000 | loss 2.9487\n",
      "step  513 / 1000 | loss 3.1072\n",
      "step  514 / 1000 | loss 2.9394\n",
      "step  515 / 1000 | loss 2.6195\n",
      "step  516 / 1000 | loss 2.9254\n",
      "step  517 / 1000 | loss 2.9090\n",
      "step  518 / 1000 | loss 2.7686\n",
      "step  519 / 1000 | loss 2.7431\n",
      "step  520 / 1000 | loss 3.2295\n",
      "step  521 / 1000 | loss 3.9015\n",
      "step  522 / 1000 | loss 3.7276\n",
      "step  523 / 1000 | loss 3.1295\n",
      "step  524 / 1000 | loss 2.9084\n",
      "step  525 / 1000 | loss 3.1550\n",
      "step  526 / 1000 | loss 2.9451\n",
      "step  527 / 1000 | loss 3.5071\n",
      "step  528 / 1000 | loss 3.5544\n",
      "step  529 / 1000 | loss 3.0642\n",
      "step  530 / 1000 | loss 2.7822\n",
      "step  531 / 1000 | loss 2.4944\n",
      "step  532 / 1000 | loss 2.6997\n",
      "step  533 / 1000 | loss 2.9767\n",
      "step  534 / 1000 | loss 3.7252\n",
      "step  535 / 1000 | loss 2.7930\n",
      "step  536 / 1000 | loss 3.0296\n",
      "step  537 / 1000 | loss 2.8142\n",
      "step  538 / 1000 | loss 2.9259\n",
      "step  539 / 1000 | loss 2.6904\n",
      "step  540 / 1000 | loss 2.6621\n",
      "step  541 / 1000 | loss 2.5433\n",
      "step  542 / 1000 | loss 2.7170\n",
      "step  543 / 1000 | loss 2.5642\n",
      "step  544 / 1000 | loss 3.0768\n",
      "step  545 / 1000 | loss 2.7491\n",
      "step  546 / 1000 | loss 2.8571\n",
      "step  547 / 1000 | loss 3.2066\n",
      "step  548 / 1000 | loss 2.3873\n",
      "step  549 / 1000 | loss 3.4546\n",
      "step  550 / 1000 | loss 2.9632\n",
      "step  551 / 1000 | loss 2.6739\n",
      "step  552 / 1000 | loss 3.0302\n",
      "step  553 / 1000 | loss 2.6484\n",
      "step  554 / 1000 | loss 2.8923\n",
      "step  555 / 1000 | loss 3.0308\n",
      "step  556 / 1000 | loss 2.9603\n",
      "step  557 / 1000 | loss 3.1587\n",
      "step  558 / 1000 | loss 3.0098\n",
      "step  559 / 1000 | loss 2.8208\n",
      "step  560 / 1000 | loss 2.7362\n",
      "step  561 / 1000 | loss 3.0054\n",
      "step  562 / 1000 | loss 3.6910\n",
      "step  563 / 1000 | loss 3.2535\n",
      "step  564 / 1000 | loss 3.0149\n",
      "step  565 / 1000 | loss 3.5365\n",
      "step  566 / 1000 | loss 2.7182\n",
      "step  567 / 1000 | loss 3.2118\n",
      "step  568 / 1000 | loss 2.6792\n",
      "step  569 / 1000 | loss 2.8221\n",
      "step  570 / 1000 | loss 2.8533\n",
      "step  571 / 1000 | loss 3.4957\n",
      "step  572 / 1000 | loss 3.4437\n",
      "step  573 / 1000 | loss 2.7396\n",
      "step  574 / 1000 | loss 2.8747\n",
      "step  575 / 1000 | loss 2.9872\n",
      "step  576 / 1000 | loss 2.7927\n",
      "step  577 / 1000 | loss 2.7652\n",
      "step  578 / 1000 | loss 2.6411\n",
      "step  579 / 1000 | loss 2.6333\n",
      "step  580 / 1000 | loss 2.9417\n",
      "step  581 / 1000 | loss 2.4148\n",
      "step  582 / 1000 | loss 3.7139\n",
      "step  583 / 1000 | loss 2.6111\n",
      "step  584 / 1000 | loss 2.7373\n",
      "step  585 / 1000 | loss 3.9792\n",
      "step  586 / 1000 | loss 2.9071\n",
      "step  587 / 1000 | loss 3.3215\n",
      "step  588 / 1000 | loss 2.9183\n",
      "step  589 / 1000 | loss 2.8946\n",
      "step  590 / 1000 | loss 3.7184\n",
      "step  591 / 1000 | loss 3.1390\n",
      "step  592 / 1000 | loss 3.6731\n",
      "step  593 / 1000 | loss 2.9471\n",
      "step  594 / 1000 | loss 3.5713\n",
      "step  595 / 1000 | loss 3.2732\n",
      "step  596 / 1000 | loss 2.4911\n",
      "step  597 / 1000 | loss 3.2473\n",
      "step  598 / 1000 | loss 3.0863\n",
      "step  599 / 1000 | loss 3.3013\n",
      "step  600 / 1000 | loss 3.5160\n",
      "step  601 / 1000 | loss 3.3903\n",
      "step  602 / 1000 | loss 3.0069\n",
      "step  603 / 1000 | loss 3.5102\n",
      "step  604 / 1000 | loss 3.1321\n",
      "step  605 / 1000 | loss 2.8749\n",
      "step  606 / 1000 | loss 3.4710\n",
      "step  607 / 1000 | loss 4.2038\n",
      "step  608 / 1000 | loss 2.9534\n",
      "step  609 / 1000 | loss 3.6196\n",
      "step  610 / 1000 | loss 2.9991\n",
      "step  611 / 1000 | loss 3.1724\n",
      "step  612 / 1000 | loss 3.0302\n",
      "step  613 / 1000 | loss 2.9642\n",
      "step  614 / 1000 | loss 3.3749\n",
      "step  615 / 1000 | loss 3.6363\n",
      "step  616 / 1000 | loss 3.3925\n",
      "step  617 / 1000 | loss 3.3534\n",
      "step  618 / 1000 | loss 3.3558\n",
      "step  619 / 1000 | loss 3.2479\n",
      "step  620 / 1000 | loss 3.0909\n",
      "step  621 / 1000 | loss 3.3944\n",
      "step  622 / 1000 | loss 2.8847\n",
      "step  623 / 1000 | loss 3.6421\n",
      "step  624 / 1000 | loss 3.2297\n",
      "step  625 / 1000 | loss 3.0790\n",
      "step  626 / 1000 | loss 3.8787\n",
      "step  627 / 1000 | loss 3.2741\n",
      "step  628 / 1000 | loss 4.4258\n",
      "step  629 / 1000 | loss 3.1319\n",
      "step  630 / 1000 | loss 3.6257\n",
      "step  631 / 1000 | loss 3.0349\n",
      "step  632 / 1000 | loss 3.8067\n",
      "step  633 / 1000 | loss 3.1827\n",
      "step  634 / 1000 | loss 2.8388\n",
      "step  635 / 1000 | loss 3.1982\n",
      "step  636 / 1000 | loss 2.9312\n",
      "step  637 / 1000 | loss 3.1234\n",
      "step  638 / 1000 | loss 3.7252\n",
      "step  639 / 1000 | loss 3.4742\n",
      "step  640 / 1000 | loss 3.4053\n",
      "step  641 / 1000 | loss 2.7318\n",
      "step  642 / 1000 | loss 3.0175\n",
      "step  643 / 1000 | loss 3.7674\n",
      "step  644 / 1000 | loss 3.0237\n",
      "step  645 / 1000 | loss 3.9214\n",
      "step  646 / 1000 | loss 3.6398\n",
      "step  647 / 1000 | loss 3.1783\n",
      "step  648 / 1000 | loss 4.4157\n",
      "step  649 / 1000 | loss 3.1601\n",
      "step  650 / 1000 | loss 3.0502\n",
      "step  651 / 1000 | loss 2.7039\n",
      "step  652 / 1000 | loss 3.5872\n",
      "step  653 / 1000 | loss 2.5555\n",
      "step  654 / 1000 | loss 2.6903\n",
      "step  655 / 1000 | loss 2.2089\n",
      "step  656 / 1000 | loss 2.9426\n",
      "step  657 / 1000 | loss 3.4438\n",
      "step  658 / 1000 | loss 3.0868\n",
      "step  659 / 1000 | loss 3.6566\n",
      "step  660 / 1000 | loss 3.7542\n",
      "step  661 / 1000 | loss 3.3800\n",
      "step  662 / 1000 | loss 4.0003\n",
      "step  663 / 1000 | loss 3.5821\n",
      "step  664 / 1000 | loss 3.3229\n",
      "step  665 / 1000 | loss 3.4380\n",
      "step  666 / 1000 | loss 2.8459\n",
      "step  667 / 1000 | loss 3.3832\n",
      "step  668 / 1000 | loss 3.5393\n",
      "step  669 / 1000 | loss 3.9575\n",
      "step  670 / 1000 | loss 3.8693\n",
      "step  671 / 1000 | loss 3.0793\n",
      "step  672 / 1000 | loss 3.4771\n",
      "step  673 / 1000 | loss 3.7555\n",
      "step  674 / 1000 | loss 2.6584\n",
      "step  675 / 1000 | loss 3.2068\n",
      "step  676 / 1000 | loss 3.3170\n",
      "step  677 / 1000 | loss 3.7944\n",
      "step  678 / 1000 | loss 2.9310\n",
      "step  679 / 1000 | loss 3.1681\n",
      "step  680 / 1000 | loss 3.3013\n",
      "step  681 / 1000 | loss 3.4916\n",
      "step  682 / 1000 | loss 3.6047\n",
      "step  683 / 1000 | loss 3.0364\n",
      "step  684 / 1000 | loss 3.2788\n",
      "step  685 / 1000 | loss 3.4001\n",
      "step  686 / 1000 | loss 2.4384\n",
      "step  687 / 1000 | loss 3.0657\n",
      "step  688 / 1000 | loss 2.8600\n",
      "step  689 / 1000 | loss 2.8416\n",
      "step  690 / 1000 | loss 3.8868\n",
      "step  691 / 1000 | loss 4.0341\n",
      "step  692 / 1000 | loss 3.0656\n",
      "step  693 / 1000 | loss 4.0790\n",
      "step  694 / 1000 | loss 3.1927\n",
      "step  695 / 1000 | loss 3.0739\n",
      "step  696 / 1000 | loss 3.0142\n",
      "step  697 / 1000 | loss 2.9102\n",
      "step  698 / 1000 | loss 4.1736\n",
      "step  699 / 1000 | loss 3.5123\n",
      "step  700 / 1000 | loss 3.6111\n",
      "step  701 / 1000 | loss 3.1276\n",
      "step  702 / 1000 | loss 3.2360\n",
      "step  703 / 1000 | loss 4.4545\n",
      "step  704 / 1000 | loss 3.3646\n",
      "step  705 / 1000 | loss 3.4210\n",
      "step  706 / 1000 | loss 3.0733\n",
      "step  707 / 1000 | loss 3.5148\n",
      "step  708 / 1000 | loss 4.5292\n",
      "step  709 / 1000 | loss 5.1926\n",
      "step  710 / 1000 | loss 3.9936\n",
      "step  711 / 1000 | loss 3.7401\n",
      "step  712 / 1000 | loss 3.3777\n",
      "step  713 / 1000 | loss 3.7480\n",
      "step  714 / 1000 | loss 3.9577\n",
      "step  715 / 1000 | loss 2.9778\n",
      "step  716 / 1000 | loss 2.9029\n",
      "step  717 / 1000 | loss 3.3058\n",
      "step  718 / 1000 | loss 2.8192\n",
      "step  719 / 1000 | loss 3.3766\n",
      "step  720 / 1000 | loss 3.8424\n",
      "step  721 / 1000 | loss 3.2245\n",
      "step  722 / 1000 | loss 3.6078\n",
      "step  723 / 1000 | loss 3.6460\n",
      "step  724 / 1000 | loss 3.3186\n",
      "step  725 / 1000 | loss 4.0264\n",
      "step  726 / 1000 | loss 2.2085\n",
      "step  727 / 1000 | loss 2.9650\n",
      "step  728 / 1000 | loss 3.0768\n",
      "step  729 / 1000 | loss 3.6664\n",
      "step  730 / 1000 | loss 3.4387\n",
      "step  731 / 1000 | loss 3.3500\n",
      "step  732 / 1000 | loss 3.9098\n",
      "step  733 / 1000 | loss 3.5703\n",
      "step  734 / 1000 | loss 3.1021\n",
      "step  735 / 1000 | loss 3.2300\n",
      "step  736 / 1000 | loss 3.6486\n",
      "step  737 / 1000 | loss 3.2701\n",
      "step  738 / 1000 | loss 4.2259\n",
      "step  739 / 1000 | loss 3.6793\n",
      "step  740 / 1000 | loss 2.9989\n",
      "step  741 / 1000 | loss 3.4442\n",
      "step  742 / 1000 | loss 3.5688\n",
      "step  743 / 1000 | loss 2.9715\n",
      "step  744 / 1000 | loss 4.1689\n",
      "step  745 / 1000 | loss 3.2664\n",
      "step  746 / 1000 | loss 3.1596\n",
      "step  747 / 1000 | loss 3.1048\n",
      "step  748 / 1000 | loss 3.3354\n",
      "step  749 / 1000 | loss 3.2489\n",
      "step  750 / 1000 | loss 4.6373\n",
      "step  751 / 1000 | loss 3.0215\n",
      "step  752 / 1000 | loss 3.2734\n",
      "step  753 / 1000 | loss 4.1498\n",
      "step  754 / 1000 | loss 2.7372\n",
      "step  755 / 1000 | loss 4.2093\n",
      "step  756 / 1000 | loss 3.3692\n",
      "step  757 / 1000 | loss 3.3533\n",
      "step  758 / 1000 | loss 2.9767\n",
      "step  759 / 1000 | loss 3.6186\n",
      "step  760 / 1000 | loss 3.5263\n",
      "step  761 / 1000 | loss 3.7063\n",
      "step  762 / 1000 | loss 3.6203\n",
      "step  763 / 1000 | loss 3.9474\n",
      "step  764 / 1000 | loss 3.9543\n",
      "step  765 / 1000 | loss 3.1269\n",
      "step  766 / 1000 | loss 3.4412\n",
      "step  767 / 1000 | loss 3.9694\n",
      "step  768 / 1000 | loss 4.2944\n",
      "step  769 / 1000 | loss 2.9848\n",
      "step  770 / 1000 | loss 2.6143\n",
      "step  771 / 1000 | loss 3.2023\n",
      "step  772 / 1000 | loss 3.2557\n",
      "step  773 / 1000 | loss 2.5415\n",
      "step  774 / 1000 | loss 3.1149\n",
      "step  775 / 1000 | loss 3.8690\n",
      "step  776 / 1000 | loss 4.9879\n",
      "step  777 / 1000 | loss 4.3338\n",
      "step  778 / 1000 | loss 2.4911\n",
      "step  779 / 1000 | loss 3.8295\n",
      "step  780 / 1000 | loss 4.2236\n",
      "step  781 / 1000 | loss 3.4589\n",
      "step  782 / 1000 | loss 2.9249\n",
      "step  783 / 1000 | loss 3.4794\n",
      "step  784 / 1000 | loss 2.8685\n",
      "step  785 / 1000 | loss 3.0646\n",
      "step  786 / 1000 | loss 3.3147\n",
      "step  787 / 1000 | loss 3.9316\n",
      "step  788 / 1000 | loss 4.0763\n",
      "step  789 / 1000 | loss 2.7936\n",
      "step  790 / 1000 | loss 3.1492\n",
      "step  791 / 1000 | loss 3.1752\n",
      "step  792 / 1000 | loss 2.9931\n",
      "step  793 / 1000 | loss 3.5439\n",
      "step  794 / 1000 | loss 2.8890\n",
      "step  795 / 1000 | loss 3.2332\n",
      "step  796 / 1000 | loss 3.5285\n",
      "step  797 / 1000 | loss 3.5726\n",
      "step  798 / 1000 | loss 3.1448\n",
      "step  799 / 1000 | loss 3.0825\n",
      "step  800 / 1000 | loss 2.8314\n",
      "step  801 / 1000 | loss 3.4537\n",
      "step  802 / 1000 | loss 3.5430\n",
      "step  803 / 1000 | loss 2.6441\n",
      "step  804 / 1000 | loss 3.2431\n",
      "step  805 / 1000 | loss 3.9044\n",
      "step  806 / 1000 | loss 3.4851\n",
      "step  807 / 1000 | loss 3.6495\n",
      "step  808 / 1000 | loss 3.5291\n",
      "step  809 / 1000 | loss 3.7718\n",
      "step  810 / 1000 | loss 3.1274\n",
      "step  811 / 1000 | loss 3.7738\n",
      "step  812 / 1000 | loss 3.5662\n",
      "step  813 / 1000 | loss 3.3384\n",
      "step  814 / 1000 | loss 2.8543\n",
      "step  815 / 1000 | loss 3.4652\n",
      "step  816 / 1000 | loss 3.0034\n",
      "step  817 / 1000 | loss 3.5112\n",
      "step  818 / 1000 | loss 3.8995\n",
      "step  819 / 1000 | loss 3.8659\n",
      "step  820 / 1000 | loss 3.1524\n",
      "step  821 / 1000 | loss 3.0591\n",
      "step  822 / 1000 | loss 3.1097\n",
      "step  823 / 1000 | loss 3.1804\n",
      "step  824 / 1000 | loss 3.3697\n",
      "step  825 / 1000 | loss 3.0508\n",
      "step  826 / 1000 | loss 3.2698\n",
      "step  827 / 1000 | loss 3.4237\n",
      "step  828 / 1000 | loss 3.1231\n",
      "step  829 / 1000 | loss 3.9993\n",
      "step  830 / 1000 | loss 3.5180\n",
      "step  831 / 1000 | loss 3.2657\n",
      "step  832 / 1000 | loss 4.3869\n",
      "step  833 / 1000 | loss 4.4659\n",
      "step  834 / 1000 | loss 2.9128\n",
      "step  835 / 1000 | loss 3.6282\n",
      "step  836 / 1000 | loss 3.9798\n",
      "step  837 / 1000 | loss 3.3619\n",
      "step  838 / 1000 | loss 3.0717\n",
      "step  839 / 1000 | loss 2.8733\n",
      "step  840 / 1000 | loss 2.4793\n",
      "step  841 / 1000 | loss 3.1513\n",
      "step  842 / 1000 | loss 2.9353\n",
      "step  843 / 1000 | loss 3.8252\n",
      "step  844 / 1000 | loss 3.2319\n",
      "step  845 / 1000 | loss 3.8270\n",
      "step  846 / 1000 | loss 2.8686\n",
      "step  847 / 1000 | loss 3.8423\n",
      "step  848 / 1000 | loss 2.9674\n",
      "step  849 / 1000 | loss 2.9549\n",
      "step  850 / 1000 | loss 2.9143\n",
      "step  851 / 1000 | loss 3.1535\n",
      "step  852 / 1000 | loss 3.9335\n",
      "step  853 / 1000 | loss 2.6275\n",
      "step  854 / 1000 | loss 2.9425\n",
      "step  855 / 1000 | loss 2.9985\n",
      "step  856 / 1000 | loss 2.9339\n",
      "step  857 / 1000 | loss 3.1043\n",
      "step  858 / 1000 | loss 3.1327\n",
      "step  859 / 1000 | loss 2.8424\n",
      "step  860 / 1000 | loss 2.6718\n",
      "step  861 / 1000 | loss 3.0819\n",
      "step  862 / 1000 | loss 3.6586\n",
      "step  863 / 1000 | loss 3.1146\n",
      "step  864 / 1000 | loss 3.1992\n",
      "step  865 / 1000 | loss 2.9701\n",
      "step  866 / 1000 | loss 3.4753\n",
      "step  867 / 1000 | loss 3.7411\n",
      "step  868 / 1000 | loss 3.0891\n",
      "step  869 / 1000 | loss 3.2435\n",
      "step  870 / 1000 | loss 3.6056\n",
      "step  871 / 1000 | loss 3.9751\n",
      "step  872 / 1000 | loss 3.1803\n",
      "step  873 / 1000 | loss 5.0479\n",
      "step  874 / 1000 | loss 3.4809\n",
      "step  875 / 1000 | loss 2.9534\n",
      "step  876 / 1000 | loss 3.3722\n",
      "step  877 / 1000 | loss 2.5126\n",
      "step  878 / 1000 | loss 3.3433\n",
      "step  879 / 1000 | loss 3.3606\n",
      "step  880 / 1000 | loss 3.2820\n",
      "step  881 / 1000 | loss 3.5092\n",
      "step  882 / 1000 | loss 3.5015\n",
      "step  883 / 1000 | loss 4.0937\n",
      "step  884 / 1000 | loss 2.9425\n",
      "step  885 / 1000 | loss 3.2013\n",
      "step  886 / 1000 | loss 3.4223\n",
      "step  887 / 1000 | loss 3.2028\n",
      "step  888 / 1000 | loss 3.3704\n",
      "step  889 / 1000 | loss 2.8502\n",
      "step  890 / 1000 | loss 3.3681\n",
      "step  891 / 1000 | loss 2.8477\n",
      "step  892 / 1000 | loss 2.8484\n",
      "step  893 / 1000 | loss 3.3126\n",
      "step  894 / 1000 | loss 3.3135\n",
      "step  895 / 1000 | loss 3.0559\n",
      "step  896 / 1000 | loss 2.9363\n",
      "step  897 / 1000 | loss 3.4562\n",
      "step  898 / 1000 | loss 2.5817\n",
      "step  899 / 1000 | loss 2.9302\n",
      "step  900 / 1000 | loss 3.1090\n",
      "step  901 / 1000 | loss 3.2691\n",
      "step  902 / 1000 | loss 3.0570\n",
      "step  903 / 1000 | loss 3.0895\n",
      "step  904 / 1000 | loss 3.2608\n",
      "step  905 / 1000 | loss 3.4303\n",
      "step  906 / 1000 | loss 3.9089\n",
      "step  907 / 1000 | loss 3.3932\n",
      "step  908 / 1000 | loss 3.3962\n",
      "step  909 / 1000 | loss 4.0386\n",
      "step  910 / 1000 | loss 3.2389\n",
      "step  911 / 1000 | loss 3.1697\n",
      "step  912 / 1000 | loss 3.0459\n",
      "step  913 / 1000 | loss 3.4538\n",
      "step  914 / 1000 | loss 3.0569\n",
      "step  915 / 1000 | loss 3.3991\n",
      "step  916 / 1000 | loss 2.8210\n",
      "step  917 / 1000 | loss 3.3875\n",
      "step  918 / 1000 | loss 2.9756\n",
      "step  919 / 1000 | loss 2.9617\n",
      "step  920 / 1000 | loss 3.3464\n",
      "step  921 / 1000 | loss 3.0692\n",
      "step  922 / 1000 | loss 2.5676\n",
      "step  923 / 1000 | loss 3.6658\n",
      "step  924 / 1000 | loss 3.0188\n",
      "step  925 / 1000 | loss 3.0267\n",
      "step  926 / 1000 | loss 3.1131\n",
      "step  927 / 1000 | loss 2.6206\n",
      "step  928 / 1000 | loss 4.1656\n",
      "step  929 / 1000 | loss 3.1697\n",
      "step  930 / 1000 | loss 3.5397\n",
      "step  931 / 1000 | loss 3.0994\n",
      "step  932 / 1000 | loss 3.2806\n",
      "step  933 / 1000 | loss 3.2514\n",
      "step  934 / 1000 | loss 3.5475\n",
      "step  935 / 1000 | loss 3.4970\n",
      "step  936 / 1000 | loss 3.4777\n",
      "step  937 / 1000 | loss 4.4449\n",
      "step  938 / 1000 | loss 2.8394\n",
      "step  939 / 1000 | loss 2.7162\n",
      "step  940 / 1000 | loss 3.2925\n",
      "step  941 / 1000 | loss 3.3476\n",
      "step  942 / 1000 | loss 3.7708\n",
      "step  943 / 1000 | loss 4.0493\n",
      "step  944 / 1000 | loss 3.1265\n",
      "step  945 / 1000 | loss 3.0532\n",
      "step  946 / 1000 | loss 3.4337\n",
      "step  947 / 1000 | loss 3.6860\n",
      "step  948 / 1000 | loss 3.0423\n",
      "step  949 / 1000 | loss 3.8700\n",
      "step  950 / 1000 | loss 2.9449\n",
      "step  951 / 1000 | loss 2.7687\n",
      "step  952 / 1000 | loss 3.5399\n",
      "step  953 / 1000 | loss 3.3459\n",
      "step  954 / 1000 | loss 2.8245\n",
      "step  955 / 1000 | loss 3.2508\n",
      "step  956 / 1000 | loss 2.9615\n",
      "step  957 / 1000 | loss 3.1321\n",
      "step  958 / 1000 | loss 3.0431\n",
      "step  959 / 1000 | loss 3.1515\n",
      "step  960 / 1000 | loss 3.1847\n",
      "step  961 / 1000 | loss 2.4842\n",
      "step  962 / 1000 | loss 3.2272\n",
      "step  963 / 1000 | loss 3.6505\n",
      "step  964 / 1000 | loss 3.3972\n",
      "step  965 / 1000 | loss 2.8936\n",
      "step  966 / 1000 | loss 3.2884\n",
      "step  967 / 1000 | loss 2.9575\n",
      "step  968 / 1000 | loss 3.2074\n",
      "step  969 / 1000 | loss 2.3400\n",
      "step  970 / 1000 | loss 3.0881\n",
      "step  971 / 1000 | loss 3.7722\n",
      "step  972 / 1000 | loss 3.2716\n",
      "step  973 / 1000 | loss 3.9223\n",
      "step  974 / 1000 | loss 2.5067\n",
      "step  975 / 1000 | loss 3.1873\n",
      "step  976 / 1000 | loss 3.2800\n",
      "step  977 / 1000 | loss 2.9792\n",
      "step  978 / 1000 | loss 3.4781\n",
      "step  979 / 1000 | loss 3.1640\n",
      "step  980 / 1000 | loss 2.7690\n",
      "step  981 / 1000 | loss 3.9416\n",
      "step  982 / 1000 | loss 3.7880\n",
      "step  983 / 1000 | loss 2.7706\n",
      "step  984 / 1000 | loss 3.5386\n",
      "step  985 / 1000 | loss 3.4676\n",
      "step  986 / 1000 | loss 2.7766\n",
      "step  987 / 1000 | loss 2.9054\n",
      "step  988 / 1000 | loss 3.3065\n",
      "step  989 / 1000 | loss 2.7474\n",
      "step  990 / 1000 | loss 3.3970\n",
      "step  991 / 1000 | loss 3.5029\n",
      "step  992 / 1000 | loss 2.8654\n",
      "step  993 / 1000 | loss 4.0474\n",
      "step  994 / 1000 | loss 3.1332\n",
      "step  995 / 1000 | loss 3.5252\n",
      "step  996 / 1000 | loss 3.0073\n",
      "step  997 / 1000 | loss 2.9630\n",
      "step  998 / 1000 | loss 3.1143\n",
      "step  999 / 1000 | loss 3.7655\n",
      "step 1000 / 1000 | loss 3.4828\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "beta_1 = .85\n",
    "beta_2 = .99\n",
    "eps_adam = 1e-8\n",
    "\n",
    "steps = 1000\n",
    "\n",
    "m1 = [0 for _ in params]\n",
    "m2 = [0 for _ in params]\n",
    "\n",
    "for step in range(steps):\n",
    "  sonnet = train_ds[step]['text']\n",
    "  tokens = [tokenizer[c] for c in list(sonnet)]\n",
    "  n = min(block_size, len(tokens) - 1)\n",
    "\n",
    "  keys, values = [[] for _ in range(layers)], [[] for _ in range(layers)]\n",
    "  losses = []\n",
    "  for pos_id in range(n):\n",
    "    token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
    "    logits = gpt(token_id, pos_id, keys, values)\n",
    "    probs = softmax(logits)\n",
    "    loss_t = -probs[target_id].log() # cross entropy, only count - 1 * log(p)\n",
    "    losses.append(loss_t)\n",
    "\n",
    "  loss = sum(losses, Value(0.0)) / Value(float(n))\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  lr_t = lr * (1 - step / steps)\n",
    "  for i, p in enumerate(params):\n",
    "    m1[i] = beta_1 * m1[i] + (1 - beta_1) * p.grad\n",
    "    m2[i] = beta_2 * m2[i] + (1 - beta_2) * p.grad ** 2\n",
    "\n",
    "    m1_hat = m1[i] / (1 - beta_1 ** (step + 1))\n",
    "    m2_hat = m2[i] / (1 - beta_2 ** (step + 1))\n",
    "\n",
    "    p.val -= lr_t * m1_hat / (m2_hat ** 0.5 + eps_adam) ## fast update\n",
    "\n",
    "    p.grad = 0 # zero gradients at end \n",
    "\n",
    "  print(f\"step {step+1:4d} / {steps:4d} | loss {loss.val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea152cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a87337b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1334e8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 59\n",
      "target_id 4\n",
      "len(logits) 59\n",
      "len(probs) 59\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size', vocab_size)\n",
    "print('target_id', target_id)\n",
    "print('len(logits)', len(logits))\n",
    "print('len(probs)', len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "0ed8eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## See if the model overfits with the forward pass\n",
    "\n",
    "context_text = \"Love's f\"\n",
    "inputs = list(context_text)\n",
    "tokens = [tokenizer[c] for c in list(context_text)]\n",
    "\n",
    "temp = 0.5\n",
    "\n",
    "for sample_idx in range(20):\n",
    "  keys, values = [[] for _ in range(layers)], [[] for _ in range(layers)]\n",
    "  token_id = 5\n",
    "  sample = []\n",
    "  for pos_id in range(block_size):\n",
    "    logits = gpt(token_id, pos_id, keys, values)\n",
    "    probs = softmax([l / temp for l in logits])\n",
    "\n",
    "    token_id = random.choices(range(vocab_size), weights=[p.val for p in probs])[0]\n",
    "\n",
    "    sample += [token_id]\n",
    "\n",
    "full_txt = ''.join([vocab[c] for c in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "45770d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  thaluo annda l'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f8187",
   "metadata": {},
   "source": [
    "Yeah so this is complete nonsense. Try again on list of names. That's definitely much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee319b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
